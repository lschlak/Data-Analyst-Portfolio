{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan: \n",
    "1. Do overall sentiment analysis with nltk (and get price from the Factset datafeed, and then graph sentiment + stock price over time?)\n",
    "2. Do keynes testing per document with nltk\n",
    "3. Do BERTopic modeling on the sentences? pages? \n",
    "4. Could I do the sentiment analysis on each page or sentence and then tie that back to the original source? Say, for instance, the topic is \"government\" and the sentiment is general negative? Maybe use nltk or VADER? \n",
    "5. Then use AI to give a summary of the page? \n",
    "6. Put in company names as stop words? \n",
    "\n",
    "*Automated pipeline: \n",
    "You put in the transcripts (get from Factset or Bloomberg), automated pipeline produces a) summary of transcript, \n",
    "This should all be hosted in a Python Shiny or node js. Or maybe it just outputs a nice little text file with a: \n",
    "1. Title\n",
    "2. Brief AI Summary (use RAG?)\n",
    "3. Overall Sentiment Score  \n",
    "4. Keynes Testing Output\n",
    "5. Topic Modeling + Sentiment Scores per Topic [with GPT generated topic names...maybe pull out the finacial topics and see which companies have the worst sentiment score? Use few shot prompting] - How do I choose the number of topics...?\n",
    "6. Wordcloud generated\n",
    "7. Use ticker as check for sentiment analysis? \n",
    "\n",
    "\n",
    "for chunking: \n",
    "https://www.geeksforgeeks.org/how-to-chunk-text-data-a-comparative-analysis/\n",
    "\n",
    "\n",
    "#Create a node.js application:  \n",
    "https://plotly.com/nodejs/line-and-scatter/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################\n",
    "# Package Installation: \n",
    "#########################################################################################################################################\n",
    "\n",
    "#%pip install transformers torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "\n",
    "# Import torch first to avoid circular import issues\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import math\n",
    "import nltk\n",
    "import ssl\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "from keyness import log_likelihood\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "from nltk.text import Text  \n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Import vader sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Import vader sentiment\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import argparse\n",
    "from json import JSONDecodeError\n",
    "import os\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import sys\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve API key\n",
    "with open(\"C:\\\\Users\\\\lschlake\\\\secrets.txt\") as f:\n",
    "    secrets = f.read()\n",
    "secrets = json.loads(secrets)\n",
    "\n",
    "#Get API key from json\n",
    "api_key= secrets['azure_api_key']\n",
    "\n",
    "#Set parameters for model\n",
    "rand_api_base = \"https://apigw.rand.org/openai/RAND/inference/\"\n",
    "rand_api_version = '2024-02-01'\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = secrets['azure_api_key']\n",
    "header = {\"Ocp-Apim-Subscription-Key\": secrets['azure_api_key']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define two functions to be used for text splitting\n",
    "\n",
    "#Function for chunking text into chunks based on a defined chunk size\n",
    "def chunk_text(text, chunk_size):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "#If you use the chunk_text function, be sure you set the chunk size (in characters): \n",
    "cs = 800\n",
    "\n",
    "#Function to split text into paragraphs. Use if you want to do pargraph splitting: \n",
    "def split_text(text):\n",
    "    # Split the text wherever there is \" \\n \\n\"\n",
    "    split_parts = text.split(\" \\n \\n\")\n",
    "    return split_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcript:\n",
    "    def __init__(self, filepath, ticker):\n",
    "        self.filepath = filepath\n",
    "        self.ticker = ticker\n",
    "\n",
    "    #Always run the processing functions first to convert PDF --> text, chunk text, and do VADER sentiment analysis\n",
    "    def processing(self): \n",
    "\n",
    "        #Store text from each transcripts here:\n",
    "        corpus_text = []\n",
    "\n",
    "        #The text of each document\n",
    "        each_document_text = []\n",
    "\n",
    "        #Initialize sentiment score\n",
    "        sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "        #Dataframe for sentiment scores and initialize sentiment score function\n",
    "        sentiment_table = pd.DataFrame(columns=[\"file\",\"document\",\"chunk\",'vader_pos_score', \"vader_neg_score\", \"vader_neutral_score\", \"vader_compound\"])\n",
    "\n",
    "        #Stores text of each PDF\n",
    "        indv_doc_text=\"\"\n",
    "\n",
    "        #Path to PDF:\n",
    "        path = self.filepath\n",
    "\n",
    "        #Read in PDF as text\n",
    "        reader = PdfReader(path)\n",
    "\n",
    "        #For each page in PDF, extract the text and add it to the document_text variable\n",
    "        #If you want to do this for each PDF in a folder, put a line of code here that loops through the folder\n",
    "        #Then indent the rest of the code to run subordinate to the loop\n",
    "        document_text = \"\"\n",
    "        for x in range(len(reader.pages)):\n",
    "            page = reader.pages[x]\n",
    "            document_text = document_text + page.extract_text()\n",
    "            #document_text.replace(\"wwwcallstreetcom_copyright\",\"\")\n",
    "        each_document_text.append(document_text)\n",
    "\n",
    "        #Chunks the document one of three ways:\n",
    "        paragraph_chunk = sent_tokenize(document_text) #For sentence chunking\n",
    "        #paragraph_chunk = split_text(document_text) #For pargraph chunking\n",
    "        #paragraph_chunk = split_text(document_text) #For naive chunking\n",
    "\n",
    "\n",
    "        #For each text chunk, get sentiment scores using VADER Sentiment and store those in the sentiment_table dataframe\n",
    "        for i, paragraph in enumerate(paragraph_chunk):\n",
    "\n",
    "\n",
    "            substring = \"www.callstreet.com\"\n",
    "\n",
    "            #Avoids processing sentences with \"www.callstreet.com\"\n",
    "            if substring not in paragraph:\n",
    "\n",
    "                #Append each chunk to the corpus text\n",
    "                indv_doc_text = indv_doc_text+paragraph\n",
    "                corpus_text.append(paragraph)\n",
    "\n",
    "                #Get sentiment score of the paragraph\n",
    "                score = sentiment_analyzer.polarity_scores(paragraph)\n",
    "\n",
    "                #Sets the sentiment scores and adds them to the sentiment table\n",
    "                pos_score=score[\"pos\"]\n",
    "                neg_score=score[\"neg\"]\n",
    "                neutral_score=score[\"neu\"]\n",
    "                compound_score= score[\"compound\"]\n",
    "                new_row = pd.DataFrame({\"file\": f\"{self.ticker}\",\"document\": [x],\n",
    "                                                    \"chunk\": [paragraph],\n",
    "                                                    \"vader_pos_score\":[pos_score],\n",
    "                                                    \"vader_neg_score\":[neg_score] ,\n",
    "                                                    \"vader_neutral_score\":[neutral_score] ,\n",
    "                                                    \"vader_compound\": [compound_score]})\n",
    "                \n",
    "                #Adds a new row to the sentiment table for each sentence\n",
    "                sentiment_table = pd.concat([sentiment_table, new_row], ignore_index=True)\n",
    "\n",
    "        #Set some characteristics of the class to use in other functions:          \n",
    "        self.dataframe = sentiment_table\n",
    "        self.corpus = corpus_text\n",
    "        self.indv_doc_text=indv_doc_text\n",
    "\n",
    "    #Returns the sentiment table\n",
    "    def get_sentiment(self):\n",
    "\n",
    "        #Return the top sentiment scores \n",
    "        return self.dataframe\n",
    "    \n",
    "    #Gets the BERTopics\n",
    "    def get_topics(self, cluster_number):\n",
    "        self.cluster = cluster_number\n",
    "        if self.corpus is None:\n",
    "            raise \"You must run preprocessing before running the topic model\"\n",
    "        \n",
    "        vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "        cluster_model = KMeans(n_clusters=cluster_number)\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            hdbscan_model=cluster_model\n",
    "            #Specify an embedding model\n",
    "            #Specify a clustering model\n",
    "        )\n",
    "\n",
    "        #Fit the BERTopic model\n",
    "        topics, probs = topic_model.fit_transform(self.corpus)\n",
    "        topic_names = topic_model.topic_labels_\n",
    "\n",
    "                \n",
    "        #Define prompt\n",
    "        names = \"\"\"\n",
    "        This is a list of topics from a BERTopic model that analyses text from earnings call transcripts of public companies.\n",
    "          Please generate new names for each of these topics and return a list. \n",
    "          Seperate the topic numbers from the topic names using a colon.  \n",
    "          Output the original topic number along with the name. \n",
    "          Put each topic name on a seprate line. Include no other text in your response.\"\"\"\n",
    "        \n",
    "        #Sets the promt\n",
    "        topics = topic_model.topic_labels_\n",
    "        for x in topics: \n",
    "            names = names + topics[x] + \"; \"\n",
    "        # Set deployment ID based on model argument.\n",
    "\n",
    "        #If necessary, specify your deployment id here:\n",
    "        #deployment_id = \n",
    "\n",
    "        #Define you API endpoint here\n",
    "        #endpoint =  \n",
    "\n",
    "        chat_completion_response = requests.post(\n",
    "                endpoint,\n",
    "                headers = {'Ocp-Apim-Subscription-Key': api_key},\n",
    "                json = {\n",
    "                    'messages': [\n",
    "                        {\n",
    "                            'role': 'user',\n",
    "                            'content': names\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        #Retrieve the GPT-generate new names:\n",
    "        response_deserialized_content = chat_completion_response.json()\n",
    "        result = response_deserialized_content['choices'][0]['message']['content'].splitlines()\n",
    "        #Store the new names in a dataframe:\n",
    "        new_topic_names = pd.DataFrame(result)\n",
    "        #Rename the axis as the topic number: \n",
    "        new_topic_names=new_topic_names.set_axis(['Topic'], axis=1)\n",
    "        #Split the topic numbers off from the topic name:\n",
    "        new_topic_names[[\"Topic\",\"New Topic Name\"]]=new_topic_names[\"Topic\"].str.split(\":\", n=1, expand=True)\n",
    "        #Convert the topic number to a numeric variable:\n",
    "        new_topic_names[\"Topic\"]= pd.to_numeric(new_topic_names[\"Topic\"], downcast='integer', errors='coerce')\n",
    "        #Store the BERTopic model output in a data table:\n",
    "        topic_categories = pd.DataFrame(topic_model.get_document_info(self.corpus))\n",
    "        #Merge the BERTtopic model table and the sentiment scores based on the category number\n",
    "        table1 = pd.merge(topic_categories, self.dataframe, how=\"outer\", left_index=True, right_index=True)\n",
    "        #Merge the BERTopic model/sentiment score tables and the new topic names table\n",
    "        table2 = pd.merge(table1,new_topic_names, how=\"right\", on=\"Topic\")\n",
    "\n",
    "        #print(f\"{response_deserialized_content['choices'][0]['message']['content']}\\n\")\n",
    "        return table2.groupby(\"New Topic Name\")[\"vader_compound\"].mean()\n",
    "\n",
    "    #Gets a summary for the text\n",
    "    def get_summary(self):\n",
    "        import time\n",
    "        \n",
    "        #If necessary, specify your deployment id here:\n",
    "        #deployment_id = \n",
    "\n",
    "        #Define you API endpoint here\n",
    "        #endpoint =  \n",
    "                    \n",
    "        x= self.indv_doc_text\n",
    "        time.sleep(2)\n",
    "        prompt = \"Produce a short summary of this text. Include a description of key points and the sentiment of each of those points:\" + x\n",
    "\n",
    "                #print('Sending request to Azure OpenAI Chat Completions endpoint...')\n",
    "\n",
    "        chat_completion_response = requests.post(\n",
    "                        endpoint,\n",
    "                        headers = {'Ocp-Apim-Subscription-Key': api_key},\n",
    "                        json = {\n",
    "                            'messages': [\n",
    "                                {\n",
    "                                    'role': 'user',\n",
    "                                    'content': prompt\n",
    "                                },\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "        response_deserialized_content = chat_completion_response.json()\n",
    "        print(f\"{response_deserialized_content['choices'][0]['message']['content']}\\n\")\n",
    "\n",
    "\n",
    "    #Does AI sentiment analysis\n",
    "    def get_ai_sentiment(self):\n",
    "        #Code in development for sentiment analysis using RANDChat API:\n",
    "        #Get ticker from yahoo finance?\n",
    "\n",
    "        #If necessary, specify your deployment id here:\n",
    "        #deployment_id = \n",
    "        #Define you API endpoint here\n",
    "        #endpoint =  \n",
    "\n",
    "        AI_sentiment= pd.DataFrame(columns=[\"chunk_number\",\"sentiment_score\"])\n",
    "        paragraph_number = 1\n",
    "\n",
    "        for x in self.corpus: \n",
    "                time.sleep(3)\n",
    "                context = \"\"\"\n",
    "                I want you to perform a sentiment analysis on sentences. \n",
    "                Here are some examples of how to go about the sentiment analysis. The sentiment scores should range from \n",
    "                -1 (completely negative) to 1 (completely positive). 0 represents a sentiment nuetral chunk. \n",
    "\n",
    "                Example #1\n",
    "                Chunk: \"Our revenue exceeded expectations and all projections from the previous quarter.\" \n",
    "                Sentiment Score: 0.8\n",
    "\n",
    "\n",
    "                Example #2\n",
    "                Chunk: \"We are experienced a slowdown in our supply chain as a result of geopolitical tension.\"\n",
    "                Sentiment Score: -0.6\n",
    "\n",
    "                Example #3\n",
    "                Chunk: \"All forward-looking statements are merely projects\"\n",
    "                Sentiment Score: 0.0\n",
    "                \"\"\"\n",
    "\n",
    "                prompt = context + \" . Now classify this body of text. Only return the sentiment score in your respond. Do not include any other text: \" + x\n",
    "\n",
    "                chat_completion_response = requests.post(\n",
    "                        endpoint,\n",
    "                        headers = {'Ocp-Apim-Subscription-Key': api_key},\n",
    "                        json = {\n",
    "                            'messages': [\n",
    "                                {\n",
    "                                    'role': 'user',\n",
    "                                    'content': prompt\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                response_deserialized_content2 = chat_completion_response.json()\n",
    "                #print(x)\n",
    "                #print(f\"{paragraph_number},{response_deserialized_content2['choices'][0]['message']['content']}\\n\")\n",
    "                new_row = pd.DataFrame({\"chunk_number\":paragraph_number,\"sentiment_score\":[response_deserialized_content2['choices'][0]['message']['content']]})\n",
    "                AI_sentiment = pd.concat([AI_sentiment, new_row], ignore_index=True)\n",
    "                paragraph_number += 1\n",
    "                print(\"Loading Sentence #\", paragraph_number)\n",
    "\n",
    "        return AI_sentiment \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = Transcript(\"C:\\\\Users\\\\lschlake\\\\Documents\\\\2024-2025Courses\\\\Text Analysis\\\\Project\\\\Real Transcripts\\\\L3Harris Technologies Inc (2).pdf\", \"L3 Harris\")\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "print(f\"           {d1.ticker} Report                 \")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "d1.processing()\n",
    "d1.get_summary()\n",
    "d1.get_topics(30) #Runs the get topic function, specifies the KMeans model to create 30 topics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
